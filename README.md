# landmark-retrieval
## Abstract
In this paper, we extend two deep learning methods to perform image retrieval on Google Landmark Dataset v2. Image retrieval provides an efficient method that helps to search for related visual information within a large database. Image retrieval can be formatted as a representation learning problem where we construct an embedding space for querying similar landmark images. Our network uses ResNet-101 pre-trained on ImageNet for feature extraction and we apply two representation learning algorithms to maximize the intra-class compactness and the inter-class discrepancy of the embeddings extracted from the landmark images. We demonstrate that these two algorithms can be transferred to the image retrieval task beyond its application in the original domain, and one of the algorithms ArcFace obtains a superior retrieval performance.

## Introduction
The primary goal of image retrieval is to query a base image by analyzing the relevance of all the contents in an image database and collecting data that is similar to the base image. A large-scale benchmark, the Google Landmarks Dataset v2 (GLDv2)can be used to evaluate the performance and generalization of image retrieval techniques, and it contains more than 5 million images of human-made and natural landmarks worldwide. 

In this paper, we implement two representation learning algorithms for landmark image retrieval. The first algorithm is inspired by \emph{Generalized End-to-End Loss for Speaker Verification}, which was proposed to perform speaker verification by leveraging the centroids of the embedding vectors for different speakers to find representative clusters. The second algorithm, \emph{Additive Angular Margin Loss (ArcFace)} \cite{deng2019arcface}, adds an angular margin to the angle between the features and target weights in each dimension of class, which modifies the cross entropy loss to achieve more distinguishable embeddings. To compare the two algorithms, we use the same ResNet-101network pre-trained on ImageNet as the encoder before the representation learning stage. We also design a variant of U-Netas the baseline to learn low-dimensional embeddings during image reconstruction.

